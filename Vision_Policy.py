import torch
import torch.nn as nn
from torch.amp import autocast
from matplotlib import gridspec
import numpy as np
from scipy.stats import norm
import copy
import math
import matplotlib.pyplot as plt
import torch.nn.functional as F
from Model import CustomModel

class Policy(nn.Module):
    def __init__(self, state_dim, action_dim, min_std=0.05, scaling_factor=3, num_quantiles=32):
        super(Policy, self).__init__()

        self.num_quantiles = num_quantiles
        self.model = CustomModel(state_dim, action_dim, num_quantiles=num_quantiles)

        self.scaling_factor = scaling_factor
        self.min_std = min_std
        self.mean_std = 0
        self.timestep = 1

    def load_policy(self, path):
        pretained_model = torch.load(path, map_location=lambda storage, loc: storage)
        self.model.load_state_dict(pretained_model['policy_state_dict'], strict=True)

    def set_policy(self, policy):
        self.model = copy.deepcopy(policy.model)

    def set_state_dict(self, state_dict):
        self.model.load_state_dict(state_dict)

    def act(self, state, memory, det=False, log=False):
        with torch.no_grad():
            with autocast('cuda'):
                memory.states.append(state.detach())
                NN_output, state_value_quantiles = self.model.forward(state.detach())

                # The following Code was generated by Chatgpt but checked analytically by hand
                # _____________________________________________________________________________
                N = NN_output.shape[0]  # batch size

                action_dim = 2

                # --- 1. Parse the NN output ---
                split = (action_dim, action_dim)
                means, pre_log_stds = torch.split(NN_output, split, dim=-1)


                # shapes:
                # logits: [N, K]
                # means: [N, K * action_dim] → reshape → [N, K, action_dim]
                # log_stds: [N, K * action_dim] → reshape → [N, K, action_dim]

                means = self.scaling_factor * nn.functional.tanh(means.view(N, action_dim))
                pre_log_stds = pre_log_stds.view(N, action_dim)
                stds = nn.functional.tanh(torch.log(1 + torch.exp(pre_log_stds / 2))) * 2 + self.min_std
                log_stds = torch.log(stds)

                

                means = means.squeeze()
                stds = stds.squeeze()  # [N, action_dim]

                # --- 4. Sample action from selected Gaussian ---
                eps = torch.randn_like(means)
                actions = means + eps * stds  # [N, action_dim]

                # --- 5. Compute log-prob of actions under full GMM ---
                # First compute per-component log probs
                # Gaussian log prob for diagonal covariance:
                # log N(a | mu, sigma) = -0.5 * sum(((a - mu)/sigma)^2 + 2*log(sigma) + log(2pi))
                means_diff = actions - means  # [N, K, action_dim]
                log_probs = -0.5 * torch.sum(means_diff ** 2 / stds ** 2 + 2 * log_stds, dim=-1) - math.log(
                    2 * math.pi)

                self.timestep += 1

            action = actions.detach()
            memory.logprobs.append(log_probs.detach())
            memory.actions.append(action)
            self.mean_std *= self.timestep - 1
            self.mean_std += stds.detach()
            self.mean_std /= self.timestep
            self.timestep += 1
            if det:
                argmax_actions = means
                return argmax_actions.detach(), state_value_quantiles, means, stds
            if log:
                return action,  state_value_quantiles, means, stds
            return action,  state_value_quantiles

    def eval_truncation(self, state):
        return self.model.forward_critic(state)

    def forward(self, states, actions):
        with autocast('cuda'):

            NN_output, state_value_quantiles = self.model.forward(states.squeeze())

            # The following Code was generated by Chatgpt but checked analytically by hand
            # _____________________________________________________________________________
            N = NN_output.shape[0]  # batch size

            action_dim = 2

            # --- 1. Parse the NN output ---
            split = (action_dim, action_dim)
            means, pre_log_stds = torch.split(NN_output, split, dim=-1)

            # shapes:
            # logits: [N, K]
            # means: [N, K * action_dim] → reshape → [N, K, action_dim]
            # log_stds: [N, K * action_dim] → reshape → [N, K, action_dim]

            means = self.scaling_factor * nn.functional.tanh(means.view(N, action_dim))
            pre_log_stds = pre_log_stds.view(N, action_dim)
            stds = nn.functional.tanh(torch.log(1 + torch.exp(pre_log_stds / 2))) * 2 + self.min_std
            log_stds = torch.log(stds)

            # --- 5. Compute log-prob of actions under full GMM ---
            # First compute per-component log probs
            # Gaussian log prob for diagonal covariance:
            # log N(a | mu, sigma) = -0.5 * sum(((a - mu)/sigma)^2 + 2*log(sigma) + log(2pi))

            means_diff = actions - means  # [N, action_dim]
            log_probs = -0.5 * torch.sum(means_diff ** 2 / stds ** 2 + 2 * log_stds, dim=-1) - math.log(
                2 * math.pi)

            est_dist_entropy = -torch.mean(log_probs)

        return log_probs, state_value_quantiles.squeeze(), est_dist_entropy


